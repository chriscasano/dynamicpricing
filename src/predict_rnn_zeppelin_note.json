{"paragraphs":[{"text":"%spark2.pyspark\nimport os\nimport sys\n\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport random\n\nimport shutil\nimport tensorflow.contrib.learn as tflearn\nimport tensorflow.contrib.layers as tflayers\nfrom tensorflow.contrib.learn.python.learn import learn_runner\nimport tensorflow.contrib.metrics as metrics\nimport tensorflow.contrib.rnn as rnn\nimport matplotlib.pyplot as plt\n\n#plt.figure()\n#(ax1, ax2) = plt.subplots(nrows = 2, ncols = 1)\n#rng = pd.date_range(start='2000', periods=224, freq='M')\n\n#ts= pd.Series(np.random.uniform(-10, 10, size=len(rng)), rng).cumsum()\n\n#ts.plot(c='b', title=\"Example Time Series\")\n#ts.plot()\n#ts.head(300)\nts = pd.Series.from_csv('/tmp/ts.csv')\n\nTS = np.array(ts)\nnum_periods = 30\nf_horizon = 1\nx_data = TS[:(len(TS) - (len(TS) % num_periods))]\nx_batches=x_data.reshape(-1, 30, 1)\ny_data = TS[1:(len(TS) - (len(TS) % num_periods)) + f_horizon]\ny_batches = y_data.reshape(-1, 30, 1)\n\nprint (len(x_batches))\nprint (x_batches.shape)\nprint (x_batches[0:2])\nprint (y_batches.shape)\nprint (y_batches[0:1])\n\ntf.reset_default_graph()\nnum_periods = 30\ninputs =1 \nhidden = 100\noutput = 1\nX = tf.placeholder(tf.float32, [None, num_periods, inputs])\ny = tf.placeholder(tf.float32, [None, num_periods, output])\nbasic_cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden, activation=tf.nn.relu)\nrnn_output, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\nlearning_rate = 0.001\nstacked_rnn_output = tf.reshape(rnn_output,[-1, hidden])\nstacked_outputs = tf.layers.dense(stacked_rnn_output, output)\noutputs = tf.reshape(stacked_outputs, [-1, num_periods, output])\n\nloss = tf.reduce_sum(tf.square(outputs -y))\noptimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\ntraining_op = optimizer.minimize(loss)\n\ndef test_data(series, forcast, num_periods):\n    test_x_setup = series[-(num_periods + forcast):]\n    testX = test_x_setup[:num_periods].reshape(-1, 30,1)\n    testY = TS[-(num_periods):].reshape(-1, 30, 1)\n    return testX, testY\n\nX_test, Y_test = test_data(TS, f_horizon, num_periods)\nprint (X_test.shape)\nprint (X_test)\n\ninit = tf.global_variables_initializer()\n\nepochs = 2000 # number of iterations or training data\n\nwith tf.Session() as sess:\n        init.run()\n        for ep in range(epochs):\n            sess.run(training_op, feed_dict={X: x_batches, y: y_batches})\n            if ep %100 == 0:\n                mse = loss.eval(feed_dict={X: x_batches, y: y_batches})\n                print(ep, \"\\tMSE\", mse)\n        y_pred= sess.run(outputs, feed_dict={X: X_test})\n        print(y_pred)\n\nplt.title(\"Predict vs Actual\", fontsize=14)\nplt.plot(pd.Series(np.ravel(Y_test)), \"bo\", markersize=10, label=\"Actual\")\nplt.plot(pd.Series(np.ravel(y_pred)), \"g.\", markersize=10, label=\"Predict\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"Time Periods\")\n\n#plt.show()","user":"admin","dateUpdated":"2018-09-06T14:00:36+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536086436400_-1974887191","id":"20180904-184036_1243162953","dateCreated":"2018-09-04T18:40:36+0000","dateStarted":"2018-09-06T14:00:36+0000","dateFinished":"2018-09-06T14:01:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1090"},{"text":"%spark2.pyspark\nimport os\nimport sys\n\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport random\n\nimport shutil\nimport tensorflow.contrib.learn as tflearn\nimport tensorflow.contrib.layers as tflayers\nfrom tensorflow.contrib.learn.python.learn import learn_runner\nimport tensorflow.contrib.metrics as metrics\nimport tensorflow.contrib.rnn as rnn\nimport matplotlib.pyplot as plt\n\n#plt.figure()\n#(ax1, ax2) = plt.subplots(nrows = 2, ncols = 1)\nrng = pd.date_range(start='2013', periods=2129, freq='D')\n\nts= pd.Series(np.random.uniform(-10, 10, size=len(rng)), rng).cumsum().abs()\n#ts= pd.Series(np.random.uniform(50, 200, size=len(rng)), rng)\nts.to_csv(path='/tmp/ts.csv')\nts.plot(c='b', title=\"Example Time Series\")\n#ts.plot()\nts.head(300)","user":"admin","dateUpdated":"2018-09-06T14:00:14+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536086619482_-358752939","id":"20180904-184339_1592285104","dateCreated":"2018-09-04T18:43:39+0000","dateStarted":"2018-09-06T14:00:15+0000","dateFinished":"2018-09-06T14:00:15+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1091"},{"text":"%spark2.pyspark\n","user":"admin","dateUpdated":"2018-09-06T03:00:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536111673654_-1200398866","id":"20180905-014113_771250594","dateCreated":"2018-09-05T01:41:13+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1092"}],"name":"Predication_RNN","id":"2DP3B17DJ","noteParams":{},"noteForms":{},"angularObjects":{"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}